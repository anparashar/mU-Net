{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjMaq+2PpD6ZTM3l1SvMzk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anparashar/mU-Net/blob/main/mU_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vx4pL3hQZxro"
      },
      "outputs": [],
      "source": [
        "## Loading Dataset\n",
        "#from simple_unet_model import simple_unet_model   #Use normal unet model\n",
        "from tensorflow.keras.utils import normalize\n",
        "#mport normalize\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "image_directory = 'image directory'\n",
        "mask_directory = 'mask directory'\n",
        "\n",
        "\n",
        "\n",
        "SIZEX = 256\n",
        "\n",
        "SIZEY= 256\n",
        "\n",
        "image_dataset = []\n",
        "mask_dataset = []\n",
        "\n",
        "#Loading images\n",
        "images = os.listdir(image_directory)\n",
        "for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
        "    if (image_name.split('.')[1] == 'jpeg'):\n",
        "        image = cv2.imread(image_directory+image_name,1)\n",
        "        image = Image.fromarray(image)\n",
        "        image = image.resize((SIZEX, SIZEY))\n",
        "        image_dataset.append(np.array(image))\n",
        "\n",
        "## Loading Mask\n",
        "\n",
        "masks = os.listdir(mask_directory)\n",
        "for i, image_name in enumerate(masks):\n",
        "    if (image_name.split('.')[1] == 'jpeg'):\n",
        "        mask = cv2.imread(mask_directory+image_name,0)\n",
        "        mask = Image.fromarray(mask)\n",
        "        mask = mask.resize((SIZEX, SIZEY))\n",
        "        mask_dataset.append(np.array(mask))\n",
        "\n",
        "\n",
        "        #Normalize images\n",
        "image_dataset = normalize(np.array(image_dataset), axis=0)\n",
        "\n",
        "mask_dataset = (np.array(mask_dataset))/255\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.20, random_state = 0)\n"
      ],
      "metadata": {
        "id": "BgrmD8AFZ_ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating train, test and validation set\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.50, random_state = 0)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "id": "xSffewaSb4-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "import random\n",
        "import numpy as np\n",
        "image_number = random.randint(0, len(X_train))\n",
        "print(image_number)\n",
        "plt.imshow(X_train[image_number])\n",
        "plt.show()\n",
        "plt.subplot(122)\n",
        "plt.imshow(y_train[image_number], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(image_dataset.shape)\n",
        "print(mask_dataset.shape)\n",
        "NofImage =image_dataset.shape[0]\n",
        "IMG_HEIGHT =image_dataset.shape[1]\n",
        "IMG_WIDTH  = image_dataset.shape[2]\n",
        "IMG_CHANNELS =image_dataset.shape[3]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a1k42C1ca1Tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mU-Net Model\n",
        "\n",
        "#!pip install segmentation-models\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D,AveragePooling2D, MaxPooling2D,GlobalAveragePooling2D, UpSampling2D,BatchNormalization, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Nadam, Adagrad, Adadelta\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#import segmentation_models as sm\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "################################################################\n",
        "\n",
        "\n",
        "def mUnet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS):\n",
        "#Build the model\n",
        "\n",
        "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
        "    s = inputs\n",
        "\n",
        "    #Contraction path\n",
        "    c1 = Conv2D(75, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
        "    c1 = Dropout(0.2)(c1)\n",
        "    c1 = Conv2D(75, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "\n",
        "    p1 = MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "    c2 = Conv2D(150, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
        "    c2 = Dropout(0.2)(c2)\n",
        "    c2 = Conv2D(150, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "\n",
        "    p2 = MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "    c3 = Conv2D(300, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
        "    c3 = Dropout(0.2)(c3)\n",
        "    c3 = Conv2D(300, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "\n",
        "    p3 = MaxPooling2D((2, 2))(c3)\n",
        "\n",
        "    c4 = Conv2D(600, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
        "    c4 = Dropout(0.2)(c4)\n",
        "    c4 = Conv2D(600, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "\n",
        "    c5 = Conv2D(1200, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
        "    c5 = Dropout(0.2)(c5)\n",
        "    c5 = Conv2D(1200, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
        "\n",
        "\n",
        "    #Expansive path\n",
        "    u6 = Conv2DTranspose(600, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "    m1= MaxPooling2D((8, 8))(c1)\n",
        "    l1= concatenate([u6, m1])\n",
        "    #k1= Conv2D(480, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
        "    u6 = concatenate([l1, c4])\n",
        "    c6 = Conv2D(600, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
        "    c6 = Dropout(0.2)(c6)\n",
        "    c6 = Conv2D(600, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
        "\n",
        "\n",
        "    u7 = Conv2DTranspose(300, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "    m2= MaxPooling2D((4, 4))(c1)\n",
        "    l2= concatenate([u7, m2])\n",
        "    #k2= Conv2D(240, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
        "    u7 = concatenate([l2, c3])\n",
        "    c7 = Conv2D(300, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
        "    c7 = Dropout(0.2)(c7)\n",
        "    c7 = Conv2D(300, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(150, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "    m3= MaxPooling2D((2, 2))(c1)\n",
        "    l3= concatenate([u8, m3])\n",
        "    #k3= Conv2D(120, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
        "    u8 = concatenate([l3, c2])\n",
        "    c8 = Conv2D(150, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
        "    c8 = Dropout(0.2)(c8)\n",
        "    c8 = Conv2D(150, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
        "\n",
        "\n",
        "    u9 = Conv2DTranspose(75, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "   # m4= MaxPooling2D((2, 2))(c1)\n",
        "    l4= concatenate([u9, c1])\n",
        "    #k4= Conv2D(60, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
        "    u9 = concatenate([l4, c1], axis=3)\n",
        "    c9 = Conv2D(75, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
        "    c9 = Dropout(0.2)(c9)\n",
        "    c9 = Conv2D(75, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
        "\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer= 'Adam' , loss= 'categorical_crossentropy')\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "a6is_k7DbLM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
        "\n",
        "# calling model function\n",
        "def get_model():\n",
        "    return mUnet_model(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "model = get_model()\n",
        "\n",
        "# Training of model\n",
        "\n",
        "def lr_decay(epoch):\n",
        "        initAlpha=0.001\n",
        "        factor=0.5\n",
        "        dropEvery=7\n",
        "        alpha=initAlpha*(factor ** np.floor((1+epoch)/dropEvery))\n",
        "        return float(alpha)\n",
        "\n",
        "callbacks=[LearningRateScheduler(lr_decay)]\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size = 8,\n",
        "                    verbose=1,\n",
        "                    epochs=150,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    callbacks=callbacks,shuffle=False)\n",
        "\n",
        "model.save('Fundus_test.hdf5')\n"
      ],
      "metadata": {
        "id": "u7lwNqcobeUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "from skimage.metrics import structural_similarity, peak_signal_noise_ratio\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error, recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "def compute_iou(im1, im2):\n",
        "    overlap = (im1>0.5) * (im2>0.5)\n",
        "    union = (im1>0.5) + (im2>0.5)\n",
        "    return overlap.sum()/float(union.sum())\n",
        "\n",
        "\n",
        "# Calculate loss\n",
        "def calculate_loss(y_true, y_pred):\n",
        "    return mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Evaluation metric: Dice\n",
        "def compute_dice(im1, im2, empty_score=1.0):\n",
        "    im1 = np.asarray(im1>0.5).astype(np.bool)\n",
        "    im2 = np.asarray(im2>0.5).astype(np.bool)\n",
        "\n",
        "    if im1.shape != im2.shape:\n",
        "        raise ValueError(\"Shape mismatch: im1 and im2 must have the same shape.\")\n",
        "\n",
        "    im_sum = im1.sum() + im2.sum()\n",
        "    if im_sum == 0:\n",
        "        return empty_score\n",
        "\n",
        "    intersection = np.logical_and(im1, im2)\n",
        "\n",
        "    return 2. * intersection.sum() / im_sum\n",
        "\n",
        "#model loading\n",
        "model.load_weights('model path')\n",
        "\n",
        "for i in range(1,50):\n",
        "    test_img = X_test[i]\n",
        "    ground_truth=y_test[i]\n",
        "    test_img=np.array(test_img, dtype=float)\n",
        "    test_img = np.expand_dims(test_img, axis=0)\n",
        "\n",
        "    prediction = (model.predict(test_img) > 0.2)\n",
        "    ground_truth_arg=np.argmax(ground_truth,axis=1)\n",
        "    prediction_arg = np.argmax(prediction,axis=1)\n",
        "    #prediction=np.argmax(prediction, axis=1)\n",
        "    #print(prediction_arg.shape)\n",
        "    #print(ground_truth_arg.shape)\n",
        "    #iou=compute_iou(prediction[0,:,:,0],ground_truth[:,:])\n",
        "    #print(iou)\n",
        "    #dice=compute_dice(prediction[0,:,:,0],ground_truth[:,:])\n",
        "    #print(dice)\n",
        "    #print(ground_truth.shape)\n",
        "    #print(prediction.shape)\n",
        "\n",
        "    #mae=mean_absolute_error(ground_truth[:,:], prediction[0,:,:,0])\n",
        "    #print(mae)\n",
        "    #ssim=structural_similarity(ground_truth[:,:], prediction[0,:,:,0])\n",
        "    #print(ssim)\n",
        "    #psnr=peak_signal_noise_ratio(ground_truth[:,:], prediction[0,:,:,0],data_range=1)\n",
        "    #print(psnr)\n",
        "    mse = mean_squared_error(ground_truth[:,:], prediction[0,:,:,0])\n",
        "    print(mse)\n",
        "    #print(\"Loss:\", loss)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lPjVgsrIcVzq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}